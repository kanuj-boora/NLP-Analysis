{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>abominable</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abominations</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>...</th>\n",
       "      <th>youthful</th>\n",
       "      <th>yowza</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yup</th>\n",
       "      <th>zaps</th>\n",
       "      <th>zealots</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E02</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E04</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E07</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 4898 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abandoned  abilities  ability  ablaze  able  abominable  abomination  \\\n",
       "E01          2          0        0       0     0           0            0   \n",
       "E02          1          0        1       0     1           0            0   \n",
       "E03          1          1        1       1     2           0            0   \n",
       "E04          1          0        1       0     1           1            0   \n",
       "E05          2          1        1       0     0           0            0   \n",
       "E06          1          0        2       0     1           0            0   \n",
       "E07          1          0        0       0     1           0            0   \n",
       "E08          1          0        1       0     1           0            1   \n",
       "\n",
       "     abominations  abruptly  absolutely  ...  youthful  yowza  yuck  yup  \\\n",
       "E01             0         0           0  ...         0      0     0    0   \n",
       "E02             0         0           0  ...         0      0     0    0   \n",
       "E03             1         1           0  ...         0      0     2    0   \n",
       "E04             0         0           1  ...         0      0     0    0   \n",
       "E05             0         0           0  ...         1      0     0    0   \n",
       "E06             0         0           0  ...         0      0     0    0   \n",
       "E07             0         0           0  ...         0      1     0    0   \n",
       "E08             1         0           0  ...         0      0     0    1   \n",
       "\n",
       "     zaps  zealots  zero  zombies  zone  zurich  \n",
       "E01     0        1     1        0     0       0  \n",
       "E02     0        0     0        0     0       0  \n",
       "E03     0        0     0        1     0       0  \n",
       "E04     0        0     2        0     2       0  \n",
       "E05     0        0     0        0     0       0  \n",
       "E06     0        0     0        0     0       0  \n",
       "E07     1        0     0        0     0       1  \n",
       "E08     0        0     1        0     0       0  \n",
       "\n",
       "[8 rows x 4898 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('./wednesday_data/dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E01</th>\n",
       "      <th>E02</th>\n",
       "      <th>E03</th>\n",
       "      <th>E04</th>\n",
       "      <th>E05</th>\n",
       "      <th>E06</th>\n",
       "      <th>E07</th>\n",
       "      <th>E08</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abilities</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ablaze</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           E01  E02  E03  E04  E05  E06  E07  E08\n",
       "abandoned    2    1    1    1    2    1    1    1\n",
       "abilities    0    0    1    0    1    0    0    0\n",
       "ability      0    1    1    1    1    2    0    1\n",
       "ablaze       0    0    1    0    0    0    0    0\n",
       "able         0    1    2    1    0    1    1    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"./wednesday_data/cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"tyler\" + 0.004*\"hyde\" + 0.004*\"monster\" + 0.004*\"crackstone\" + 0.004*\"weems\" + 0.003*\"come\" + 0.003*\"nevermore\" + 0.003*\"xavier\" + 0.003*\"time\" + 0.003*\"need\"'),\n",
       " (1,\n",
       "  '0.005*\"enid\" + 0.005*\"come\" + 0.005*\"want\" + 0.005*\"did\" + 0.004*\"nevermore\" + 0.004*\"need\" + 0.004*\"got\" + 0.004*\"oh\" + 0.004*\"going\" + 0.004*\"let\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"eugene\" + 0.000*\"enid\" + 0.000*\"need\" + 0.000*\"oh\" + 0.000*\"going\" + 0.000*\"xavier\" + 0.000*\"dance\" + 0.000*\"did\" + 0.000*\"come\" + 0.000*\"want\"'),\n",
       " (1,\n",
       "  '0.005*\"come\" + 0.005*\"nevermore\" + 0.005*\"got\" + 0.004*\"did\" + 0.004*\"let\" + 0.004*\"want\" + 0.004*\"going\" + 0.004*\"rowan\" + 0.004*\"enid\" + 0.004*\"school\"'),\n",
       " (2,\n",
       "  '0.005*\"enid\" + 0.004*\"monster\" + 0.004*\"want\" + 0.004*\"need\" + 0.004*\"oh\" + 0.004*\"tyler\" + 0.004*\"did\" + 0.003*\"think\" + 0.003*\"fester\" + 0.003*\"yeah\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"monster\" + 0.004*\"fester\" + 0.004*\"hyde\" + 0.004*\"crackstone\" + 0.004*\"need\" + 0.004*\"yeah\" + 0.004*\"want\" + 0.003*\"come\" + 0.003*\"did\" + 0.003*\"nevermore\"'),\n",
       " (1,\n",
       "  '0.007*\"enid\" + 0.007*\"tyler\" + 0.005*\"come\" + 0.005*\"xavier\" + 0.005*\"oh\" + 0.004*\"want\" + 0.004*\"eugene\" + 0.004*\"didn\" + 0.004*\"monster\" + 0.004*\"weems\"'),\n",
       " (2,\n",
       "  '0.005*\"rowan\" + 0.005*\"garrett\" + 0.005*\"father\" + 0.005*\"come\" + 0.004*\"did\" + 0.004*\"want\" + 0.004*\"let\" + 0.004*\"mother\" + 0.004*\"got\" + 0.004*\"nevermore\"'),\n",
       " (3,\n",
       "  '0.006*\"nevermore\" + 0.005*\"school\" + 0.004*\"did\" + 0.004*\"way\" + 0.004*\"rowan\" + 0.003*\"going\" + 0.003*\"want\" + 0.003*\"mother\" + 0.003*\"oh\" + 0.003*\"need\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01</th>\n",
       "      <td>Original release date  November          Wedne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E02</th>\n",
       "      <td>Original release date  November          Wedne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E03</th>\n",
       "      <td>Original release date  November          Wedne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E04</th>\n",
       "      <td>Original release date  November          Wedne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05</th>\n",
       "      <td>Original release date  November             ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06</th>\n",
       "      <td>Original release date  November          Wedne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E07</th>\n",
       "      <td>Original release date  November          At Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08</th>\n",
       "      <td>Original release date  November          Wedne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript\n",
       "E01  Original release date  November          Wedne...\n",
       "E02  Original release date  November          Wedne...\n",
       "E03  Original release date  November          Wedne...\n",
       "E04  Original release date  November          Wedne...\n",
       "E05  Original release date  November             ye...\n",
       "E06  Original release date  November          Wedne...\n",
       "E07  Original release date  November          At Ma...\n",
       "E08  Original release date  November          Wedne..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('./wednesday_data/data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01</th>\n",
       "      <td>release date November Wednesday Addams school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E02</th>\n",
       "      <td>release date November Wednesday Sheriff Galpin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E03</th>\n",
       "      <td>release date November Wednesday members studen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E04</th>\n",
       "      <td>release date November Wednesday Thing break co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05</th>\n",
       "      <td>release date November years Gomez suspicion Ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06</th>\n",
       "      <td>release date November Wednesday Goody ancestor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E07</th>\n",
       "      <td>release date November Mayor Walker Wednesday f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08</th>\n",
       "      <td>release date November Wednesday classmates Tyl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript\n",
       "E01  release date November Wednesday Addams school ...\n",
       "E02  release date November Wednesday Sheriff Galpin...\n",
       "E03  release date November Wednesday members studen...\n",
       "E04  release date November Wednesday Thing break co...\n",
       "E05  release date November years Gomez suspicion Ga...\n",
       "E06  release date November Wednesday Goody ancestor...\n",
       "E07  release date November Mayor Walker Wednesday f...\n",
       "E08  release date November Wednesday classmates Tyl..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abominations</th>\n",
       "      <th>academy</th>\n",
       "      <th>acapella</th>\n",
       "      <th>accident</th>\n",
       "      <th>accounts</th>\n",
       "      <th>accusations</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>yeti</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoko</th>\n",
       "      <th>yonder</th>\n",
       "      <th>yowza</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E02</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E04</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E07</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2810 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abilities  ability  abomination  abominations  academy  acapella  \\\n",
       "E01          0        0            0             0        1         0   \n",
       "E02          0        1            0             0        0         1   \n",
       "E03          1        1            0             1        2         0   \n",
       "E04          0        1            0             0        0         0   \n",
       "E05          1        1            0             0        0         0   \n",
       "E06          0        2            0             0        0         0   \n",
       "E07          0        0            0             0        0         0   \n",
       "E08          0        1            1             1        0         0   \n",
       "\n",
       "     accident  accounts  accusations  act  ...  yeti  yo  yoko  yonder  yowza  \\\n",
       "E01         1         0            0    0  ...     0   2     0       0      0   \n",
       "E02         2         0            0    0  ...     0   0     2       0      0   \n",
       "E03         0         0            0    0  ...     0   0     2       1      0   \n",
       "E04         1         0            0    1  ...     3   3     0       0      0   \n",
       "E05         2         1            0    0  ...     0   0     0       0      0   \n",
       "E06         0         0            0    2  ...     0   0     1       0      0   \n",
       "E07         0         0            0    0  ...     0   0     5       0      1   \n",
       "E08         0         0            1    1  ...     0   1     1       0      0   \n",
       "\n",
       "     yuck  zero  zombies  zone  zurich  \n",
       "E01     0     1        0     0       0  \n",
       "E02     0     0        0     0       0  \n",
       "E03     2     0        1     0       0  \n",
       "E04     0     1        0     1       0  \n",
       "E05     0     0        0     0       0  \n",
       "E06     0     0        0     0       0  \n",
       "E07     0     0        0     0       1  \n",
       "E08     0     1        0     0       0  \n",
       "\n",
       "[8 rows x 2810 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said', 'music', 'playing']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"wednesday\" + 0.008*\"tyler\" + 0.008*\"enid\" + 0.007*\"nevermore\" + 0.007*\"monster\" + 0.006*\"rowan\" + 0.006*\"oh\" + 0.006*\"school\" + 0.006*\"weems\" + 0.006*\"thing\"'),\n",
       " (1,\n",
       "  '0.018*\"wednesday\" + 0.008*\"garrett\" + 0.007*\"eugene\" + 0.006*\"father\" + 0.006*\"dance\" + 0.005*\"way\" + 0.005*\"oh\" + 0.005*\"monster\" + 0.005*\"gates\" + 0.004*\"night\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"wednesday\" + 0.009*\"nevermore\" + 0.007*\"fester\" + 0.007*\"school\" + 0.007*\"hyde\" + 0.005*\"tyler\" + 0.005*\"monster\" + 0.005*\"rowan\" + 0.005*\"oh\" + 0.005*\"thing\"'),\n",
       " (1,\n",
       "  '0.001*\"wednesday\" + 0.001*\"rowan\" + 0.001*\"monster\" + 0.001*\"tyler\" + 0.000*\"mother\" + 0.000*\"thing\" + 0.000*\"school\" + 0.000*\"oh\" + 0.000*\"nevermore\" + 0.000*\"enid\"'),\n",
       " (2,\n",
       "  '0.025*\"wednesday\" + 0.009*\"enid\" + 0.007*\"tyler\" + 0.007*\"monster\" + 0.006*\"oh\" + 0.006*\"weems\" + 0.006*\"nevermore\" + 0.006*\"crackstone\" + 0.006*\"eugene\" + 0.006*\"rowan\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"wednesday\" + 0.001*\"monster\" + 0.000*\"eugene\" + 0.000*\"enid\" + 0.000*\"come\" + 0.000*\"thing\" + 0.000*\"nevermore\" + 0.000*\"way\" + 0.000*\"oh\" + 0.000*\"father\"'),\n",
       " (1,\n",
       "  '0.018*\"wednesday\" + 0.010*\"eugene\" + 0.010*\"dance\" + 0.008*\"monster\" + 0.008*\"rave\" + 0.007*\"come\" + 0.007*\"goo\" + 0.006*\"oh\" + 0.005*\"enid\" + 0.005*\"thing\"'),\n",
       " (2,\n",
       "  '0.028*\"wednesday\" + 0.009*\"tyler\" + 0.009*\"enid\" + 0.008*\"nevermore\" + 0.007*\"school\" + 0.007*\"mother\" + 0.006*\"rowan\" + 0.006*\"oh\" + 0.006*\"way\" + 0.006*\"monster\"'),\n",
       " (3,\n",
       "  '0.011*\"crackstone\" + 0.009*\"wednesday\" + 0.007*\"monster\" + 0.006*\"hey\" + 0.005*\"house\" + 0.005*\"rowan\" + 0.005*\"town\" + 0.005*\"jericho\" + 0.005*\"meeting\" + 0.005*\"joseph\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01</th>\n",
       "      <td>Original release date November Wednesday Addam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E02</th>\n",
       "      <td>Original release date November Wednesday skept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E03</th>\n",
       "      <td>Original release date November Wednesday membe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E04</th>\n",
       "      <td>Original release date November Wednesday Thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05</th>\n",
       "      <td>Original release date November years Gomez sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06</th>\n",
       "      <td>Original release date November Wednesday Goody...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E07</th>\n",
       "      <td>Original release date November Mayor Walker fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08</th>\n",
       "      <td>Original release date November Wednesday class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript\n",
       "E01  Original release date November Wednesday Addam...\n",
       "E02  Original release date November Wednesday skept...\n",
       "E03  Original release date November Wednesday membe...\n",
       "E04  Original release date November Wednesday Thing...\n",
       "E05  Original release date November years Gomez sus...\n",
       "E06  Original release date November Wednesday Goody...\n",
       "E07  Original release date November Mayor Walker fu...\n",
       "E08  Original release date November Wednesday class..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abominable</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abominations</th>\n",
       "      <th>academy</th>\n",
       "      <th>acapella</th>\n",
       "      <th>accept</th>\n",
       "      <th>accident</th>\n",
       "      <th>...</th>\n",
       "      <th>yoko</th>\n",
       "      <th>yonder</th>\n",
       "      <th>young</th>\n",
       "      <th>youthful</th>\n",
       "      <th>yowza</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E02</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E04</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E07</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 3460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abilities  ability  able  abominable  abomination  abominations  academy  \\\n",
       "E01          0        0     0           0            0             0        1   \n",
       "E02          0        1     1           0            0             0        0   \n",
       "E03          1        1     2           0            0             1        2   \n",
       "E04          0        1     1           1            0             0        0   \n",
       "E05          1        1     0           0            0             0        0   \n",
       "E06          0        2     1           0            0             0        0   \n",
       "E07          0        0     1           0            0             0        0   \n",
       "E08          0        1     1           0            1             1        0   \n",
       "\n",
       "     acapella  accept  accident  ...  yoko  yonder  young  youthful  yowza  \\\n",
       "E01         0       1         1  ...     0       0      0         0      0   \n",
       "E02         1       0         2  ...     2       0      0         0      0   \n",
       "E03         0       0         0  ...     2       1      1         0      0   \n",
       "E04         0       0         1  ...     0       0      0         0      0   \n",
       "E05         0       0         2  ...     0       0      0         1      0   \n",
       "E06         0       0         0  ...     1       0      0         0      0   \n",
       "E07         0       0         0  ...     5       0      1         0      1   \n",
       "E08         0       0         0  ...     1       0      1         0      0   \n",
       "\n",
       "     yuck  zero  zombies  zone  zurich  \n",
       "E01     0     1        0     0       0  \n",
       "E02     0     0        0     0       0  \n",
       "E03     2     0        1     0       0  \n",
       "E04     0     1        0     1       0  \n",
       "E05     0     0        0     0       0  \n",
       "E06     0     0        0     0       0  \n",
       "E07     0     0        0     0       1  \n",
       "E08     0     1        0     0       0  \n",
       "\n",
       "[8 rows x 3460 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"wednesday\" + 0.011*\"tyler\" + 0.007*\"enid\" + 0.007*\"hyde\" + 0.005*\"monster\" + 0.005*\"xavier\" + 0.005*\"laurel\" + 0.005*\"gates\" + 0.005*\"weems\" + 0.005*\"oh\"'),\n",
       " (1,\n",
       "  '0.017*\"wednesday\" + 0.006*\"rowan\" + 0.006*\"nevermore\" + 0.005*\"school\" + 0.005*\"way\" + 0.005*\"oh\" + 0.005*\"monster\" + 0.004*\"enid\" + 0.004*\"mother\" + 0.004*\"thing\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"wednesday\" + 0.009*\"tyler\" + 0.007*\"enid\" + 0.007*\"gates\" + 0.006*\"hyde\" + 0.005*\"father\" + 0.005*\"oh\" + 0.005*\"monster\" + 0.005*\"garrett\" + 0.005*\"nevermore\"'),\n",
       " (1,\n",
       "  '0.015*\"wednesday\" + 0.008*\"rowan\" + 0.007*\"nevermore\" + 0.006*\"school\" + 0.005*\"enid\" + 0.005*\"thing\" + 0.004*\"way\" + 0.004*\"monster\" + 0.004*\"oh\" + 0.004*\"crackstone\"'),\n",
       " (2,\n",
       "  '0.013*\"wednesday\" + 0.007*\"eugene\" + 0.007*\"dance\" + 0.006*\"monster\" + 0.006*\"rave\" + 0.006*\"goo\" + 0.005*\"xavier\" + 0.005*\"come\" + 0.004*\"oh\" + 0.004*\"enid\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"wednesday\" + 0.007*\"enid\" + 0.007*\"nevermore\" + 0.006*\"tyler\" + 0.006*\"oh\" + 0.005*\"school\" + 0.005*\"monster\" + 0.005*\"fester\" + 0.004*\"hyde\" + 0.004*\"thing\"'),\n",
       " (1,\n",
       "  '0.014*\"wednesday\" + 0.013*\"garrett\" + 0.009*\"father\" + 0.007*\"gates\" + 0.007*\"mother\" + 0.007*\"morticia\" + 0.005*\"family\" + 0.005*\"gomez\" + 0.005*\"way\" + 0.004*\"nevermore\"'),\n",
       " (2,\n",
       "  '0.013*\"wednesday\" + 0.009*\"rowan\" + 0.006*\"crackstone\" + 0.005*\"monster\" + 0.005*\"enid\" + 0.005*\"weems\" + 0.004*\"nevermore\" + 0.004*\"thing\" + 0.004*\"good\" + 0.004*\"hey\"'),\n",
       " (3,\n",
       "  '0.023*\"wednesday\" + 0.010*\"tyler\" + 0.008*\"eugene\" + 0.008*\"xavier\" + 0.006*\"monster\" + 0.006*\"enid\" + 0.006*\"weems\" + 0.006*\"dance\" + 0.005*\"thing\" + 0.005*\"oh\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"wednesday\" + 0.006*\"nevermore\" + 0.006*\"oh\" + 0.005*\"school\" + 0.005*\"way\" + 0.005*\"thing\" + 0.005*\"eugene\" + 0.005*\"enid\" + 0.005*\"monster\" + 0.004*\"little\"'),\n",
       " (1,\n",
       "  '0.023*\"wednesday\" + 0.010*\"tyler\" + 0.009*\"enid\" + 0.007*\"gates\" + 0.007*\"garrett\" + 0.007*\"father\" + 0.006*\"mother\" + 0.006*\"weems\" + 0.006*\"oh\" + 0.005*\"nevermore\"'),\n",
       " (2,\n",
       "  '0.016*\"wednesday\" + 0.007*\"rowan\" + 0.007*\"fester\" + 0.006*\"hyde\" + 0.006*\"monster\" + 0.005*\"xavier\" + 0.005*\"thing\" + 0.005*\"kinbott\" + 0.005*\"don\" + 0.005*\"tyler\"'),\n",
       " (3,\n",
       "  '0.009*\"crackstone\" + 0.007*\"wednesday\" + 0.006*\"monster\" + 0.005*\"hey\" + 0.004*\"house\" + 0.004*\"meeting\" + 0.004*\"jericho\" + 0.004*\"town\" + 0.004*\"old\" + 0.004*\"pilgrim\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: wednesday, nevermore\n",
    "* Topic 1: tyler, enid\n",
    "* Topic 2: rowan, hyde\n",
    "* Topic 3: crackstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'E01'),\n",
       " (2, 'E02'),\n",
       " (3, 'E03'),\n",
       " (0, 'E04'),\n",
       " (1, 'E05'),\n",
       " (1, 'E06'),\n",
       " (2, 'E07'),\n",
       " (1, 'E08')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: wednesday [E01, E04]\n",
    "* Topic 1: tyler, enid [E05, E06, E08]\n",
    "* Topic 2: rowan, hyde [E02, E07]\n",
    "* Topic 3: crackstone [E03]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the proper nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NNP' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    pnouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(pnouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01</th>\n",
       "      <td>Original release date November Wednesday Addam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E02</th>\n",
       "      <td>Original release date November Wednesday skept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E03</th>\n",
       "      <td>Original release date November Wednesday membe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E04</th>\n",
       "      <td>Original release date November Wednesday Thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05</th>\n",
       "      <td>Original release date November years Gomez sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06</th>\n",
       "      <td>Original release date November Wednesday Goody...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E07</th>\n",
       "      <td>Original release date November Mayor Walker fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08</th>\n",
       "      <td>Original release date November Wednesday class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript\n",
       "E01  Original release date November Wednesday Addam...\n",
       "E02  Original release date November Wednesday skept...\n",
       "E03  Original release date November Wednesday membe...\n",
       "E04  Original release date November Wednesday Thing...\n",
       "E05  Original release date November years Gomez sus...\n",
       "E06  Original release date November Wednesday Goody...\n",
       "E07  Original release date November Mayor Walker fu...\n",
       "E08  Original release date November Wednesday class..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pnouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_pnouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abominable</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abominations</th>\n",
       "      <th>academy</th>\n",
       "      <th>acapella</th>\n",
       "      <th>accept</th>\n",
       "      <th>accident</th>\n",
       "      <th>...</th>\n",
       "      <th>yoko</th>\n",
       "      <th>yonder</th>\n",
       "      <th>young</th>\n",
       "      <th>youthful</th>\n",
       "      <th>yowza</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E02</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E04</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E07</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 3460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abilities  ability  able  abominable  abomination  abominations  academy  \\\n",
       "E01          0        0     0           0            0             0        1   \n",
       "E02          0        1     1           0            0             0        0   \n",
       "E03          1        1     2           0            0             1        2   \n",
       "E04          0        1     1           1            0             0        0   \n",
       "E05          1        1     0           0            0             0        0   \n",
       "E06          0        2     1           0            0             0        0   \n",
       "E07          0        0     1           0            0             0        0   \n",
       "E08          0        1     1           0            1             1        0   \n",
       "\n",
       "     acapella  accept  accident  ...  yoko  yonder  young  youthful  yowza  \\\n",
       "E01         0       1         1  ...     0       0      0         0      0   \n",
       "E02         1       0         2  ...     2       0      0         0      0   \n",
       "E03         0       0         0  ...     2       1      1         0      0   \n",
       "E04         0       0         1  ...     0       0      0         0      0   \n",
       "E05         0       0         2  ...     0       0      0         1      0   \n",
       "E06         0       0         0  ...     1       0      0         0      0   \n",
       "E07         0       0         0  ...     5       0      1         0      1   \n",
       "E08         0       0         0  ...     1       0      1         0      0   \n",
       "\n",
       "     yuck  zero  zombies  zone  zurich  \n",
       "E01     0     1        0     0       0  \n",
       "E02     0     0        0     0       0  \n",
       "E03     2     0        1     0       0  \n",
       "E04     0     1        0     1       0  \n",
       "E05     0     0        0     0       0  \n",
       "E06     0     0        0     0       0  \n",
       "E07     0     0        0     0       1  \n",
       "E08     0     1        0     0       0  \n",
       "\n",
       "[8 rows x 3460 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvpna = CountVectorizer(stop_words=stop_words)\n",
    "data_cvpna = cvpna.fit_transform(data_pnouns_adj.transcript)\n",
    "data_dtmpna = pd.DataFrame(data_cvpna.toarray(), columns=cvpna.get_feature_names())\n",
    "data_dtmpna.index = data_pnouns_adj.index\n",
    "data_dtmpna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpuspna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmpna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordpna = dict((v, k) for k, v in cvpna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"wednesday\" + 0.006*\"monster\" + 0.004*\"enid\" + 0.004*\"nevermore\" + 0.004*\"don\" + 0.004*\"tyler\" + 0.004*\"school\" + 0.004*\"eugene\" + 0.004*\"mother\" + 0.004*\"ellie\"'),\n",
       " (1,\n",
       "  '0.018*\"wednesday\" + 0.006*\"enid\" + 0.006*\"nevermore\" + 0.006*\"oh\" + 0.005*\"tyler\" + 0.005*\"don\" + 0.004*\"monster\" + 0.004*\"gates\" + 0.004*\"xavier\" + 0.004*\"way\"'),\n",
       " (2,\n",
       "  '0.018*\"wednesday\" + 0.006*\"enid\" + 0.005*\"xavier\" + 0.005*\"monster\" + 0.005*\"thing\" + 0.004*\"oh\" + 0.004*\"nevermore\" + 0.004*\"rowan\" + 0.004*\"ellie\" + 0.004*\"way\"'),\n",
       " (3,\n",
       "  '0.019*\"wednesday\" + 0.007*\"enid\" + 0.006*\"tyler\" + 0.005*\"monster\" + 0.005*\"nevermore\" + 0.005*\"weems\" + 0.004*\"school\" + 0.004*\"way\" + 0.004*\"rowan\" + 0.004*\"oh\"'),\n",
       " (4,\n",
       "  '0.017*\"wednesday\" + 0.005*\"tyler\" + 0.005*\"xavier\" + 0.004*\"school\" + 0.004*\"oh\" + 0.004*\"monster\" + 0.004*\"mother\" + 0.004*\"rowan\" + 0.004*\"eugene\" + 0.004*\"nevermore\"'),\n",
       " (5,\n",
       "  '0.018*\"wednesday\" + 0.006*\"nevermore\" + 0.006*\"enid\" + 0.006*\"tyler\" + 0.005*\"oh\" + 0.005*\"rowan\" + 0.005*\"thing\" + 0.004*\"mother\" + 0.004*\"don\" + 0.004*\"way\"')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldapna = models.LdaModel(corpus=corpuspna, num_topics=6, id2word=id2wordna)\n",
    "ldapna.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
